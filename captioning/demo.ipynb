{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artwork Captioning: Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qq transformers datasets torch torchvision evaluate rouge_score onedrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onedrivedownloader import download\n",
    "\n",
    "ln = \"https://unibari-my.sharepoint.com/:u:/g/personal/n_fanelli10_studenti_uniba_it/EXeIINJMf65PqLelHAsvhvcBtOSCrRdnCRO2LGGDVE08Gw?e=cs8crd\"\n",
    "download(ln, filename=\"file.zip\", unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM, MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import truecase\n",
    "import gradio as gr\n",
    "import requests\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTModel\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "from transformers import ViTImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForMultiClassification(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        multiclass_classifications: dict[str, int],\n",
    "        multilabel_classifications: dict[str, int],\n",
    "        dropout_rate: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"Initialize a ViTForMultiClassification model for multi-classification and multi-label classification.\n",
    "\n",
    "        Args:\n",
    "            multiclass_classifications (dict[str, int]): dictionary of multiclass classification with feature names and number of classes\n",
    "            multilabel_classifications (dict[str, int]): dictionary of multilabel classification with feature names and number of classes\n",
    "            multiclass_class_weights (dict[str, torch.Tensor]): dictionary of weights for each class in each multiclass classification\n",
    "        \"\"\"\n",
    "        super(ViTForMultiClassification, self).__init__()\n",
    "        self.multiclass_classifications = multiclass_classifications\n",
    "        self.multilabel_classifications = multilabel_classifications\n",
    "        self.n_classifications = len(multiclass_classifications) + len(\n",
    "            multilabel_classifications\n",
    "        )\n",
    "\n",
    "        # initialize ViT model\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\", add_pooling_layer=False)\n",
    "\n",
    "        # add final dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # initialize classification heads\n",
    "        if self.multiclass_classifications:\n",
    "            self.multiclass_fcs = nn.ModuleList(\n",
    "                [\n",
    "                    nn.Linear(self.vit.config.hidden_size, num_classes)\n",
    "                    for num_classes in multiclass_classifications.values()\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if self.multilabel_classifications:\n",
    "            self.multilabel_fcs = nn.ModuleList(\n",
    "                [\n",
    "                    nn.Linear(self.vit.config.hidden_size, num_classes)\n",
    "                    for num_classes in multilabel_classifications.values()\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # loss weights (if multitask learning)\n",
    "        if self.n_classifications > 1:\n",
    "            self.log_vars = nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    self.n_classifications, dtype=torch.float32, requires_grad=True\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            self.log_vars = None\n",
    "\n",
    "    def freeze_base_model(self, freeze: bool):\n",
    "        \"\"\"Toggle freeze/unfreeze of the ViT model.\n",
    "\n",
    "        Args:\n",
    "            freeze (bool): freeze or unfreeze\n",
    "        \"\"\"\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = not freeze\n",
    "\n",
    "    def freeze_log_vars(self, freeze: bool):\n",
    "        \"\"\"Toggle freeze/unfreeze of the log vars.\n",
    "\n",
    "        Args:\n",
    "            freeze (bool): freeze or unfreeze\n",
    "        \"\"\"\n",
    "        if freeze and self.log_vars is not None:\n",
    "            frozen_log_vars = nn.Parameter(self.log_vars.clone().detach(), requires_grad=False)\n",
    "            self.log_vars = frozen_log_vars\n",
    "\n",
    "    def forward(\n",
    "        self, pixel_values\n",
    "    ):\n",
    "        \"\"\"Forward pass for ViTForMultiClassification model.\n",
    "\n",
    "        Args:\n",
    "            pixel_values (torch.Tensor): pixel values of the images\n",
    "        \"\"\"\n",
    "        x = self.vit(pixel_values=pixel_values).last_hidden_state[:, 0]\n",
    "        x = self.dropout(x)\n",
    "        logits = None\n",
    "\n",
    "        if self.multiclass_classifications:\n",
    "            multiclass_logits = tuple(fc(x) for fc in self.multiclass_fcs)\n",
    "            logits = multiclass_logits\n",
    "        if self.multilabel_classifications:\n",
    "            multilabel_logits = tuple(fc(x) for fc in self.multilabel_fcs)\n",
    "            if logits is not None:\n",
    "                logits = logits + multilabel_logits\n",
    "            else:\n",
    "                logits = multilabel_logits\n",
    "\n",
    "        logits_dict = {\n",
    "            feature: logits[i]\n",
    "            for i, feature in enumerate(\n",
    "                list(self.multiclass_classifications.keys())\n",
    "                + list(self.multilabel_classifications.keys())\n",
    "            )\n",
    "        }\n",
    "        return logits_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTICLASS_FEATURES = (\"artist\", \"style\", \"genre\")\n",
    "MULTILABEL_FEATURES = (\"tags\", \"media\")\n",
    "\n",
    "\n",
    "def get_multiclassification_dicts():\n",
    "    \"\"\"Get multiclassification dicts.\n",
    "\n",
    "    Returns:\n",
    "        multiclass_classifications (dict): dict with number of classes for each classification feature\n",
    "    \"\"\"\n",
    "    multiclass_classifications = {}\n",
    "    multilabel_classifications = {}\n",
    "\n",
    "    for feature in MULTICLASS_FEATURES:\n",
    "        ordinal_encoder = load(Path(\"data\") / \"ordinal_encoders\" / f\"{feature}.joblib\")\n",
    "        multiclass_classifications[feature] = len(ordinal_encoder.categories_[0])\n",
    "\n",
    "    for feature in MULTILABEL_FEATURES:\n",
    "        multilabel_binarizer = load(\n",
    "            Path(\"sklearn_encoders\") / \"multilabel_binarizers\" / f\"{feature}.joblib\"\n",
    "        )\n",
    "        multilabel_classifications[feature] = len(multilabel_binarizer.classes_)\n",
    "\n",
    "    return multiclass_classifications, multilabel_classifications\n",
    "\n",
    "\n",
    "class ViTForMultiClassificationPredictor:\n",
    "    def __init__(self, model_path, device, batch_size=1):\n",
    "        self.multiclassification_dicts = get_multiclassification_dicts()\n",
    "        self.model = self.load_model(model_path, device)\n",
    "\n",
    "        self.ordinal_encoders = {}\n",
    "        self.multilabel_binarizers = {}\n",
    "        for feature in self.multiclassification_dicts[0].keys():\n",
    "            ordinal_encoder = load(\n",
    "                Path(\"sklearn_encoders\") / \"ordinal_encoders\" / f\"{feature}.joblib\"\n",
    "            )\n",
    "            self.ordinal_encoders[feature] = ordinal_encoder\n",
    "        for feature in self.multiclassification_dicts[1].keys():\n",
    "            multilabel_binarizer = load(\n",
    "                Path(\"sklearn_encoders\") / \"multilabel_binarizers\" / f\"{feature}.joblib\"\n",
    "            )\n",
    "            self.multilabel_binarizers[feature] = multilabel_binarizer\n",
    "\n",
    "        self.processor = ViTImageProcessor.from_pretrained(\n",
    "            \"google/vit-base-patch16-224-in21k\"\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_model(self, model_path, device):\n",
    "        model = ViTForMultiClassification(*self.multiclassification_dicts)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        model = model.to(device)\n",
    "        model.train(False)\n",
    "        return model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, image):\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\")[\n",
    "            \"pixel_values\"\n",
    "        ].to(self.device)\n",
    "        outputs = self.model(pixel_values)\n",
    "\n",
    "        predicted_classes = {}\n",
    "        predicted_labels = {}\n",
    "        for feature, output in outputs.items():\n",
    "            if feature in self.multiclassification_dicts[0]:\n",
    "                predicted_class = torch.argmax(output, dim=1).item()\n",
    "                predicted_class = self.ordinal_encoders[feature].inverse_transform(\n",
    "                    [[predicted_class]]\n",
    "                )[0]\n",
    "                predicted_classes[feature] = predicted_class\n",
    "            elif feature in self.multiclassification_dicts[1]:\n",
    "                predicted_labels_example = torch.where(output > 0, 1, 0).cpu().numpy()\n",
    "                predicted_labels_example = self.multilabel_binarizers[\n",
    "                    feature\n",
    "                ].inverse_transform(predicted_labels_example)[0]\n",
    "                predicted_labels[feature] = predicted_labels_example\n",
    "\n",
    "        return predicted_classes, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"microsoft/git-base\"\n",
    "PROCESSOR = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "OUTPUT_DIR = \"tutorial\"\n",
    "\n",
    "MULTICLASSIFICATION_MODEL = ViTForMultiClassificationPredictor(\n",
    "    \"models/model-20230513_121917-35.pt\",\n",
    "    DEVICE\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\"models/git_base_gs.pt\")\n",
    "MODEL.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.train(False)\n",
    "\n",
    "# load a translation model (en-it) from hf\n",
    "TRANSLATION_MODEL = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-it\")\n",
    "TRANSLATION_MODEL.to(DEVICE)\n",
    "TRANSLATION_MODEL.train(False)\n",
    "TRANSLATION_TOKENIZER = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all paths from tutorial folder\n",
    "paths = [str(os.path.join(\"data/images\", f)) for f in os.listdir(\"data/images\") if f.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize_artist(s):\n",
    "    return \" \".join([word.capitalize() for word in s.split(\"-\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclassification_prediction_to_caption(prediction):\n",
    "    caption = \"\"\n",
    "    multiclass_preds = prediction[0]\n",
    "    multilabel_preds = prediction[1]\n",
    "\n",
    "    if multiclass_preds[\"artist\"][0] != \"other\":\n",
    "        artist_pred = capitalize_artist(multiclass_preds[\"artist\"][0])\n",
    "        caption += f\"The artwork could be attributed to {artist_pred}, in the {multiclass_preds['genre'][0].capitalize()} genre, showcasing the {multiclass_preds['style'][0].capitalize()} style.\"\n",
    "    else:\n",
    "        caption += f\"The artwork could be attributed to an unknown artist, in the {multiclass_preds['genre'][0].capitalize()} genre, showcasing the {multiclass_preds['style'][0].capitalize()} style.\"\n",
    "\n",
    "    if multilabel_preds[\"tags\"]:\n",
    "        caption += f\" It is associated with the following concepts: {', '.join(multilabel_preds['tags'])}.\"\n",
    "\n",
    "    if multilabel_preds[\"media\"]:\n",
    "        caption += f\" It is presented in the medium of {', '.join(multilabel_preds['media'])}.\"\n",
    "    return caption.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gradio interface with image input and text output\n",
    "def captioning_pipeline(image, temperature, num_beams, min_length, do_sample):\n",
    "    multiclassification_preds = MULTICLASSIFICATION_MODEL.predict(image)\n",
    "    caption = multiclassification_prediction_to_caption(multiclassification_preds)\n",
    "    \n",
    "    pixel_values = PROCESSOR(images=image, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "    generated_ids = MODEL.generate(pixel_values=pixel_values, min_length=min_length, max_length=100, num_beams=num_beams, no_repeat_ngram_size=2, do_sample=do_sample, temperature=float(temperature))\n",
    "    generated_caption = PROCESSOR.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    generated_caption = truecase.get_true_case(\"It shows \" + generated_caption)\n",
    "    caption += f\" The artwork depicts{generated_caption[8:]}.\"\n",
    "\n",
    "    # translate caption to italian\n",
    "    with torch.no_grad():\n",
    "        translated_caption = TRANSLATION_MODEL.generate(**TRANSLATION_TOKENIZER(caption, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(DEVICE))\n",
    "        translated_caption = TRANSLATION_TOKENIZER.batch_decode(translated_caption, skip_special_tokens=True)[0]\n",
    "        caption_it = f\"{translated_caption[0].upper()}{translated_caption[1:]}\"\n",
    "\n",
    "    return caption, caption_it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set theme to gstaff/whiteboard\n",
    "with gr.Blocks() as demo:\n",
    "    # set title and header\n",
    "    demo.title = \"Artwork Captioning\"\n",
    "    gr.Markdown(\"# Artwork Captioning\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            image = gr.components.Image()\n",
    "            temperature = gr.components.Slider(\n",
    "                minimum=0.0,\n",
    "                maximum=2.5,\n",
    "                step=0.1,\n",
    "                value=1.0,\n",
    "                label=\"Temperature (works only if sampling is activated)\",\n",
    "            )\n",
    "            num_beams = gr.components.Slider(\n",
    "                minimum=1, maximum=5, step=1, value=4, label=\"Number of beams\"\n",
    "            )\n",
    "            min_length = gr.components.Slider(\n",
    "                minimum=5, maximum=30, step=1, value=10, label=\"Minimum length\"\n",
    "            )\n",
    "            do_sample = gr.components.Checkbox(label=\"Sampling\")\n",
    "        with gr.Column(scale=2):\n",
    "            caption = gr.components.Textbox(\n",
    "                label=\"English caption\"\n",
    "            )\n",
    "            caption_it = gr.components.Textbox(\n",
    "                label=\"Italian caption\"\n",
    "            )\n",
    "            # increase font size\n",
    "            caption.fontsize = 100\n",
    "            caption_it.fontsize = 100\n",
    "    examples = gr.Examples(\n",
    "        [[path] for path in paths],\n",
    "        inputs=[image],\n",
    "    )\n",
    "    btn = gr.Button(\"Run\")\n",
    "    btn.click(\n",
    "        captioning_pipeline,\n",
    "        inputs=[image, temperature, num_beams, min_length, do_sample],\n",
    "        outputs=[caption, caption_it],\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
